data_conf:
    queue_configs:
        - name: "abnormal_detection_1"
          address: ["192.168.15.87", 8124]
          authkey: "liujunjieabracadabra"
          queue_name: "abnormal_detection_1"
          weight: 0.88  # 采样权重
          
        # - name: "emilia"
        #   address: ["192.168.15.87", 12349] 
        #   authkey: "liujunjieabracadabra"
        #   queue_name: "get_emilia_queue"
        #   weight: 0.12
    num_workers: 1
    pin_memory: true
    batch_size: 128
    prefetch: 10 # 预取因子

    max_frames_in_batch: 2000
    batch_type: "static"  # dynamic  static
    buffer_size: 500
    max_duration: 30   # 30s
    sampling_rate: 16000

    frontend_conf:
      fs: 16000
      window: hamming
      n_mels: 80
      frame_length: 25
      frame_shift: 10
      lfr_m: 7
      lfr_n: 6
      cmvn_file: null


train_env: # 训练环境配置
  env:
    MASTER_ADDR: "localhost"
    MASTER_PORT: 10086
    WORLD_SIZE: 1
    LOCAL_RANK: 0
    RANK: 0
  bf16_run: false
  fp16_run: false

# train conf
train_conf:
    queue_flag: true  # 数据读取队列
    train_data_path: Data/models/dialect_classification/train.jsonl
    dev_data_path: Data/models/dialect_classification/test.jsonl

    ckpt_path: ''  # 检查点路径
    seed: 1998
    fp16: false  # 混合精度
    distributed: true  # 单卡训练
    dist_backend: 'nccl'

    max_epochs: 100
    accum_grad: 1
    log_interval: 10

    optim: adamw
    optim_conf:
        lr: 0.0001   # sft ->  1e-4

    scheduler: warmuplr  # 调度器类型
    scheduler_conf:
        warmup_steps: 4000    # 预热步数
        min_lr: 1e-6         # 最小学习率

    criterion: focal  # 损失函数类型
    criterion_conf:
        alpha: 0.5   # focal loss的alpha参数
        gamma: 1    # focal loss的gamma参数
        reduction: mean
    
    clip_grad: 5  # 梯度裁剪  >0时启用
    max_keep_ckpts: 3        # 最多保留的检查点数量，0或负数表示保留所有
    save_per_step: 2000    # 大于0，根据步数保存模型；小于0，根据epoch保存模型
