# fixed params
target_sample_rate: 16000  # 模型需要的采样率: 16000

data_conf:
    queue_configs:
        - name: "strong"
          address: ["192.168.15.87", 8124]
          authkey: "liujunjieabracadabra"
          queue_name: "abnormal_detection_1"
          weight: 0.88  # 采样权重
          
        # - name: "emilia"
        #   address: ["192.168.15.87", 12349] 
        #   authkey: "liujunjieabracadabra"
        #   queue_name: "get_emilia_queue"
        #   weight: 0.12
    num_workers: 1
    pin_memory: true
    batch_size: 16
    prefetch: 10 # 预取因子

    max_frames_in_batch: 2000
    batch_type: "dynamic"  # dynamic  static
    buffer_size: 100
    max_duration: 30   # 30s
    sampling_rate: !ref <target_sample_rate>
    mel_conf:
        n_fft: 512
        num_mels: 80
        hop_size: 160
        win_size: 512
        fmin: 0
        fmax: 8000
        center: False
    frontend_conf:
      fs: 16000
      window: hamming
      n_mels: 80
      frame_length: 25
      frame_shift: 10
      lfr_m: 7
      lfr_n: 6
      cmvn_file: null

train_env: # 训练环境配置
  env:
    MASTER_ADDR: "localhost"
    MASTER_PORT: 10086
    WORLD_SIZE: 1
    LOCAL_RANK: 0
    RANK: 0
  bf16_run: false
  fp16_run: false

# train conf
train_conf:
    queue_flag: true  # 数据读取队列
    ckpt_path: 'init_model.pt'  # 检查点路径
    seed: 1998
    fp16: false  # 混合精度
    distributed: false  # 单卡训练
    dist_backend: 'nccl'
    max_epochs: 100
    optim: adam
    optim_conf:
        lr: 0.0001   # sft ->  1e-5
    scheduler: warmuplr  # 调度器类型
    scheduler_conf:
        warmup_steps: 100    # 预热步数
        min_lr: 1e-6         # 最小学习率
    max_epoch: 200
    grad_clip: 5
    accum_grad: 2
    log_interval: 5
    criterion: focal  # 损失函数类型
    criterion_conf:
        alpha: 0.5   # focal loss的alpha参数
        gamma: 1    # focal loss的gamma参数
        reduction: mean
    clip_grad: 0  # 梯度裁剪  >0时启用
    max_keep_ckpts: 3        # 最多保留的检查点数量，0或负数表示保留所有
    save_per_step: 50    # 大于0，根据步数保存模型；小于0，根据epoch保存模型
